{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Generators"
      ],
      "metadata": {
        "id": "uE-zGLRsCNd0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "khPtPWSZ3Ezf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "\n",
        "\n",
        "\n",
        "# Définir la taille cible des images pour ResNet50 (224x224)\n",
        "target_size = (224, 224)\n",
        "\n",
        "def resize_image(input_image_path, output_image_path, size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Redimensionne l'image à la taille spécifiée et la sauvegarde.\n",
        "\n",
        "    Args:\n",
        "        input_image_path (str): Chemin de l'image d'entrée.\n",
        "        output_image_path (str): Chemin pour sauvegarder l'image redimensionnée.\n",
        "        size (tuple, optionnel): Nouvelle taille de l'image. Par défaut (224, 224).\n",
        "    \"\"\"\n",
        "    with Image.open(input_image_path) as img:\n",
        "        resized_img = img.resize(size, Image.LANCZOS) # Méthodes de Redimensionnement LANCZOS\n",
        "        resized_img.save(output_image_path)\n",
        "\n",
        "# Fonction pour générer des images augmentées pour une seule classe\n",
        "def generate_augmented_images(image_path, class_name, num_images=200, save_dir='images_data'):\n",
        "    \"\"\"\n",
        "    Génère et sauvegarde des images augmentées pour une classe donnée.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Chemin vers l'image originale.\n",
        "        class_name (str): Nom de la classe (par exemple, 'dog', 'cat').\n",
        "        num_images (int, optionnel): Nombre d'images augmentées à générer. Par défaut, 100.\n",
        "        save_dir (str, optionnel): Répertoire où sauvegarder les images augmentées. Par défaut, 'preview'.\n",
        "    \"\"\"\n",
        "    # Charger et redimensionner l'image\n",
        "    img = load_img(image_path, target_size=target_size)\n",
        "    # Convertir en tableau NumPy\n",
        "    x = img_to_array(img)\n",
        "    # Ajouter une dimension de batch\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    # Prétraiter pour ResNet50\n",
        "    x = preprocess_input(x)\n",
        "\n",
        "    # Définir le générateur d'images augmentées\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        brightness_range=(0.5, 1.5)\n",
        "    )\n",
        "\n",
        "    # Créer le répertoire de sauvegarde s'il n'existe pas\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    i = 0\n",
        "    for batch in datagen.flow(x, batch_size=1,\n",
        "                              save_to_dir=save_dir,\n",
        "                              save_prefix=class_name,\n",
        "                              save_format='jpeg'):\n",
        "        i += 1\n",
        "        if i >= num_images:\n",
        "            break\n",
        "\n",
        "# Générer des images augmentées pour les classes 'dog' et 'cat'\n",
        "image_paths = {'dog': 'dog.jpg', 'cat': 'cat.jpg'}  # Dictionnaire associant les noms de classe aux chemins\n",
        "\n",
        "# Répertoire pour les images redimensionnées\n",
        "input_image_path = 'images'\n",
        "resized_image_path = 'resized_images'\n",
        "generated_image_path = 'Generated_images'\n",
        "\n",
        "if not os.path.exists(resized_image_path):\n",
        "    os.makedirs(resized_image_path)\n",
        "if not os.path.exists(generated_image_path):\n",
        "    os.makedirs(generated_image_path)\n",
        "\n",
        "for class_name, image_file in image_paths.items():\n",
        "    input_img_path = os.path.join(input_image_path, image_file)\n",
        "    output_img_path = os.path.join(resized_image_path, f'{class_name}.jpg')\n",
        "    resize_image(input_image_path=input_img_path, output_image_path=output_img_path)\n",
        "    generate_augmented_images(image_path=output_img_path, class_name=class_name, save_dir=generated_image_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constants"
      ],
      "metadata": {
        "id": "HEokHLpAGIis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed for our Cats & Dogs classes\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "# Fixed for Cats & Dogs color images\n",
        "#CHANNELS = 3\n",
        "\n",
        "IMAGE_RESIZE = 224\n",
        "RESNET50_POOLING_AVERAGE = 'avg'\n",
        "DENSE_LAYER_ACTIVATION = 'softmax'\n",
        "OBJECTIVE_FUNCTION = 'categorical_crossentropy'\n",
        "\n",
        "# Common accuracy metric for all outputs, but can use different metrics for different output\n",
        "LOSS_METRICS = ['accuracy']\n",
        "\n",
        "# EARLY_STOP_PATIENCE must be < NUM_EPOCHS\n",
        "NUM_EPOCHS = 10\n",
        "EARLY_STOP_PATIENCE = 3\n",
        "\n",
        "# These steps value should be proper FACTOR of no.-of-images in train & valid folders respectively\n",
        "# Training images processed in each step would be no.-of-train-images / STEPS_PER_EPOCH_TRAINING\n",
        "STEPS_PER_EPOCH_TRAINING = 10\n",
        "STEPS_PER_EPOCH_VALIDATION = 10\n",
        "\n",
        "# These steps value should be proper FACTOR of no.-of-images in train & valid folders respectively\n",
        "# NOTE that these BATCH* are for Keras ImageDataGenerator batching to fill epoch step input\n",
        "BATCH_SIZE_TRAINING = 100\n",
        "BATCH_SIZE_VALIDATION = 100\n",
        "\n",
        "# Using 1 to easily manage mapping between test_generator & prediction for submission preparation\n",
        "BATCH_SIZE_TESTING = 1\n",
        "\n",
        "TRAIN_DIR = 'Generated_images'"
      ],
      "metadata": {
        "id": "EhsI0PwmGI7j"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning Network Model\n",
        "**xyz_tf_kernels.h5** : Ce fichier de poids est utile pour la prédiction pure d'une image de test. Il utilise les poids pré-entraînés de ResNet50, ce qui signifie que la prédiction dépend entièrement des connaissances de ResNet50 et ne nécessite pas d'apprentissage supplémentaire de notre part.\n",
        "\n",
        "**xyz_tf_kernels_NOTOP.h5** : Ce fichier de poids est utilisé pour le transfert d'apprentissage. Il exclut la couche supérieure (TOP) de ResNet50. Ces poids sont utilisés comme poids initiaux pour l'entraînement d'une nouvelle couche (ou plusieurs couches) sur de nouvelles images d'entraînement. En utilisant ces poids, on peut tirer parti des connaissances préalables de ResNet50 tout en adaptant le modèle à un nouvel ensemble de données ou à une tâche spécifique.\n",
        "\n",
        "Ainsi, dans ce contexte, on utilise les poids xyz_tf_kernels_NOTOP.h5 pour ajuster le modèle ResNet50 à une nouvelle tâche, en conservant la majeure partie de l'apprentissage antérieur tout en permettant à une partie du modèle d'être spécifique à la nouvelle tâche.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "geGPk_jjCHqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Chargement des poids de ResNet50 à partir du fichier spécifique\n",
        "\n",
        "resnet_weights_path = 'xyz_tf_kernels_NOTOP.h5'\n",
        "\n",
        "# Création du modèle\n",
        "model = Sequential()\n",
        "\n",
        "# Ajout de la première couche à mon modèle : ajout ResNet50 au modèle\n",
        "model.add(ResNet50(include_top=False, pooling=RESNET50_POOLING_AVERAGE, weights='imagenet'))\n",
        "\n",
        "# Ajout de la seconde couche Dense pour la classification\n",
        "model.add(Dense(NUM_CLASSES, activation=DENSE_LAYER_ACTIVATION))\n",
        "\n",
        "# Désactivation de l'entraînement de la première couche (ResNet) car elle est déjà entraînée\n",
        "#--> Cela implique de toutes les couches de ResNet : model.layers[0].trainable = False\n",
        "\n",
        "# pour désactiver une couche à l interieur de ResNet50, par exemple la couche input\n",
        "model.layers[0].layers[0].trainable = False\n",
        "#pour désactvier la derbier couche de ResNet50\n",
        "#model.layers[0].layers[len(model.layers[0].layers)-1].trainable = False\n",
        "\n",
        "# Diviser les données en ensembles d'entraînement et de validation\n",
        "train_dir = 'Generated_images/train'\n",
        "test_dir = 'Generated_images/test'\n",
        "train_images = [os.path.join(train_dir, img) for img in os.listdir(train_dir)]\n",
        "test_images = [os.path.join(test_dir, img) for img in os.listdir(test_dir)]\n",
        "train_images, val_images = train_test_split(train_images, test_size=0.2, random_state=42)\n",
        "\n",
        "# Créer des générateurs d'images pour l'entraînement, la validation et le test\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'Generated_images/train',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    'Generated_images/validation',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    'Generated_images/test',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Entraînement du modèle avec validation\n",
        "model.fit(train_generator, epochs=10, validation_data=val_generator)\n",
        "\n",
        "# Évaluation du modèle sur les données de test\n",
        "loss, accuracy = model.evaluate(test_generator)\n",
        "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
        "# Affichage des couches de ResNet50\n",
        "for i, layer in enumerate(model.layers[0].layers):\n",
        "    print(f\"Layer {i}: {layer.name} - Trainable: {layer.trainable}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YlpoMAOCA2X",
        "outputId": "b1193eec-d976-4277-812c-1b7cf562ef10"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 0s 0us/step\n",
            "Layer 0: input_1 - Trainable: False\n",
            "Layer 1: conv1_pad - Trainable: True\n",
            "Layer 2: conv1_conv - Trainable: True\n",
            "Layer 3: conv1_bn - Trainable: True\n",
            "Layer 4: conv1_relu - Trainable: True\n",
            "Layer 5: pool1_pad - Trainable: True\n",
            "Layer 6: pool1_pool - Trainable: True\n",
            "Layer 7: conv2_block1_1_conv - Trainable: True\n",
            "Layer 8: conv2_block1_1_bn - Trainable: True\n",
            "Layer 9: conv2_block1_1_relu - Trainable: True\n",
            "Layer 10: conv2_block1_2_conv - Trainable: True\n",
            "Layer 11: conv2_block1_2_bn - Trainable: True\n",
            "Layer 12: conv2_block1_2_relu - Trainable: True\n",
            "Layer 13: conv2_block1_0_conv - Trainable: True\n",
            "Layer 14: conv2_block1_3_conv - Trainable: True\n",
            "Layer 15: conv2_block1_0_bn - Trainable: True\n",
            "Layer 16: conv2_block1_3_bn - Trainable: True\n",
            "Layer 17: conv2_block1_add - Trainable: True\n",
            "Layer 18: conv2_block1_out - Trainable: True\n",
            "Layer 19: conv2_block2_1_conv - Trainable: True\n",
            "Layer 20: conv2_block2_1_bn - Trainable: True\n",
            "Layer 21: conv2_block2_1_relu - Trainable: True\n",
            "Layer 22: conv2_block2_2_conv - Trainable: True\n",
            "Layer 23: conv2_block2_2_bn - Trainable: True\n",
            "Layer 24: conv2_block2_2_relu - Trainable: True\n",
            "Layer 25: conv2_block2_3_conv - Trainable: True\n",
            "Layer 26: conv2_block2_3_bn - Trainable: True\n",
            "Layer 27: conv2_block2_add - Trainable: True\n",
            "Layer 28: conv2_block2_out - Trainable: True\n",
            "Layer 29: conv2_block3_1_conv - Trainable: True\n",
            "Layer 30: conv2_block3_1_bn - Trainable: True\n",
            "Layer 31: conv2_block3_1_relu - Trainable: True\n",
            "Layer 32: conv2_block3_2_conv - Trainable: True\n",
            "Layer 33: conv2_block3_2_bn - Trainable: True\n",
            "Layer 34: conv2_block3_2_relu - Trainable: True\n",
            "Layer 35: conv2_block3_3_conv - Trainable: True\n",
            "Layer 36: conv2_block3_3_bn - Trainable: True\n",
            "Layer 37: conv2_block3_add - Trainable: True\n",
            "Layer 38: conv2_block3_out - Trainable: True\n",
            "Layer 39: conv3_block1_1_conv - Trainable: True\n",
            "Layer 40: conv3_block1_1_bn - Trainable: True\n",
            "Layer 41: conv3_block1_1_relu - Trainable: True\n",
            "Layer 42: conv3_block1_2_conv - Trainable: True\n",
            "Layer 43: conv3_block1_2_bn - Trainable: True\n",
            "Layer 44: conv3_block1_2_relu - Trainable: True\n",
            "Layer 45: conv3_block1_0_conv - Trainable: True\n",
            "Layer 46: conv3_block1_3_conv - Trainable: True\n",
            "Layer 47: conv3_block1_0_bn - Trainable: True\n",
            "Layer 48: conv3_block1_3_bn - Trainable: True\n",
            "Layer 49: conv3_block1_add - Trainable: True\n",
            "Layer 50: conv3_block1_out - Trainable: True\n",
            "Layer 51: conv3_block2_1_conv - Trainable: True\n",
            "Layer 52: conv3_block2_1_bn - Trainable: True\n",
            "Layer 53: conv3_block2_1_relu - Trainable: True\n",
            "Layer 54: conv3_block2_2_conv - Trainable: True\n",
            "Layer 55: conv3_block2_2_bn - Trainable: True\n",
            "Layer 56: conv3_block2_2_relu - Trainable: True\n",
            "Layer 57: conv3_block2_3_conv - Trainable: True\n",
            "Layer 58: conv3_block2_3_bn - Trainable: True\n",
            "Layer 59: conv3_block2_add - Trainable: True\n",
            "Layer 60: conv3_block2_out - Trainable: True\n",
            "Layer 61: conv3_block3_1_conv - Trainable: True\n",
            "Layer 62: conv3_block3_1_bn - Trainable: True\n",
            "Layer 63: conv3_block3_1_relu - Trainable: True\n",
            "Layer 64: conv3_block3_2_conv - Trainable: True\n",
            "Layer 65: conv3_block3_2_bn - Trainable: True\n",
            "Layer 66: conv3_block3_2_relu - Trainable: True\n",
            "Layer 67: conv3_block3_3_conv - Trainable: True\n",
            "Layer 68: conv3_block3_3_bn - Trainable: True\n",
            "Layer 69: conv3_block3_add - Trainable: True\n",
            "Layer 70: conv3_block3_out - Trainable: True\n",
            "Layer 71: conv3_block4_1_conv - Trainable: True\n",
            "Layer 72: conv3_block4_1_bn - Trainable: True\n",
            "Layer 73: conv3_block4_1_relu - Trainable: True\n",
            "Layer 74: conv3_block4_2_conv - Trainable: True\n",
            "Layer 75: conv3_block4_2_bn - Trainable: True\n",
            "Layer 76: conv3_block4_2_relu - Trainable: True\n",
            "Layer 77: conv3_block4_3_conv - Trainable: True\n",
            "Layer 78: conv3_block4_3_bn - Trainable: True\n",
            "Layer 79: conv3_block4_add - Trainable: True\n",
            "Layer 80: conv3_block4_out - Trainable: True\n",
            "Layer 81: conv4_block1_1_conv - Trainable: True\n",
            "Layer 82: conv4_block1_1_bn - Trainable: True\n",
            "Layer 83: conv4_block1_1_relu - Trainable: True\n",
            "Layer 84: conv4_block1_2_conv - Trainable: True\n",
            "Layer 85: conv4_block1_2_bn - Trainable: True\n",
            "Layer 86: conv4_block1_2_relu - Trainable: True\n",
            "Layer 87: conv4_block1_0_conv - Trainable: True\n",
            "Layer 88: conv4_block1_3_conv - Trainable: True\n",
            "Layer 89: conv4_block1_0_bn - Trainable: True\n",
            "Layer 90: conv4_block1_3_bn - Trainable: True\n",
            "Layer 91: conv4_block1_add - Trainable: True\n",
            "Layer 92: conv4_block1_out - Trainable: True\n",
            "Layer 93: conv4_block2_1_conv - Trainable: True\n",
            "Layer 94: conv4_block2_1_bn - Trainable: True\n",
            "Layer 95: conv4_block2_1_relu - Trainable: True\n",
            "Layer 96: conv4_block2_2_conv - Trainable: True\n",
            "Layer 97: conv4_block2_2_bn - Trainable: True\n",
            "Layer 98: conv4_block2_2_relu - Trainable: True\n",
            "Layer 99: conv4_block2_3_conv - Trainable: True\n",
            "Layer 100: conv4_block2_3_bn - Trainable: True\n",
            "Layer 101: conv4_block2_add - Trainable: True\n",
            "Layer 102: conv4_block2_out - Trainable: True\n",
            "Layer 103: conv4_block3_1_conv - Trainable: True\n",
            "Layer 104: conv4_block3_1_bn - Trainable: True\n",
            "Layer 105: conv4_block3_1_relu - Trainable: True\n",
            "Layer 106: conv4_block3_2_conv - Trainable: True\n",
            "Layer 107: conv4_block3_2_bn - Trainable: True\n",
            "Layer 108: conv4_block3_2_relu - Trainable: True\n",
            "Layer 109: conv4_block3_3_conv - Trainable: True\n",
            "Layer 110: conv4_block3_3_bn - Trainable: True\n",
            "Layer 111: conv4_block3_add - Trainable: True\n",
            "Layer 112: conv4_block3_out - Trainable: True\n",
            "Layer 113: conv4_block4_1_conv - Trainable: True\n",
            "Layer 114: conv4_block4_1_bn - Trainable: True\n",
            "Layer 115: conv4_block4_1_relu - Trainable: True\n",
            "Layer 116: conv4_block4_2_conv - Trainable: True\n",
            "Layer 117: conv4_block4_2_bn - Trainable: True\n",
            "Layer 118: conv4_block4_2_relu - Trainable: True\n",
            "Layer 119: conv4_block4_3_conv - Trainable: True\n",
            "Layer 120: conv4_block4_3_bn - Trainable: True\n",
            "Layer 121: conv4_block4_add - Trainable: True\n",
            "Layer 122: conv4_block4_out - Trainable: True\n",
            "Layer 123: conv4_block5_1_conv - Trainable: True\n",
            "Layer 124: conv4_block5_1_bn - Trainable: True\n",
            "Layer 125: conv4_block5_1_relu - Trainable: True\n",
            "Layer 126: conv4_block5_2_conv - Trainable: True\n",
            "Layer 127: conv4_block5_2_bn - Trainable: True\n",
            "Layer 128: conv4_block5_2_relu - Trainable: True\n",
            "Layer 129: conv4_block5_3_conv - Trainable: True\n",
            "Layer 130: conv4_block5_3_bn - Trainable: True\n",
            "Layer 131: conv4_block5_add - Trainable: True\n",
            "Layer 132: conv4_block5_out - Trainable: True\n",
            "Layer 133: conv4_block6_1_conv - Trainable: True\n",
            "Layer 134: conv4_block6_1_bn - Trainable: True\n",
            "Layer 135: conv4_block6_1_relu - Trainable: True\n",
            "Layer 136: conv4_block6_2_conv - Trainable: True\n",
            "Layer 137: conv4_block6_2_bn - Trainable: True\n",
            "Layer 138: conv4_block6_2_relu - Trainable: True\n",
            "Layer 139: conv4_block6_3_conv - Trainable: True\n",
            "Layer 140: conv4_block6_3_bn - Trainable: True\n",
            "Layer 141: conv4_block6_add - Trainable: True\n",
            "Layer 142: conv4_block6_out - Trainable: True\n",
            "Layer 143: conv5_block1_1_conv - Trainable: True\n",
            "Layer 144: conv5_block1_1_bn - Trainable: True\n",
            "Layer 145: conv5_block1_1_relu - Trainable: True\n",
            "Layer 146: conv5_block1_2_conv - Trainable: True\n",
            "Layer 147: conv5_block1_2_bn - Trainable: True\n",
            "Layer 148: conv5_block1_2_relu - Trainable: True\n",
            "Layer 149: conv5_block1_0_conv - Trainable: True\n",
            "Layer 150: conv5_block1_3_conv - Trainable: True\n",
            "Layer 151: conv5_block1_0_bn - Trainable: True\n",
            "Layer 152: conv5_block1_3_bn - Trainable: True\n",
            "Layer 153: conv5_block1_add - Trainable: True\n",
            "Layer 154: conv5_block1_out - Trainable: True\n",
            "Layer 155: conv5_block2_1_conv - Trainable: True\n",
            "Layer 156: conv5_block2_1_bn - Trainable: True\n",
            "Layer 157: conv5_block2_1_relu - Trainable: True\n",
            "Layer 158: conv5_block2_2_conv - Trainable: True\n",
            "Layer 159: conv5_block2_2_bn - Trainable: True\n",
            "Layer 160: conv5_block2_2_relu - Trainable: True\n",
            "Layer 161: conv5_block2_3_conv - Trainable: True\n",
            "Layer 162: conv5_block2_3_bn - Trainable: True\n",
            "Layer 163: conv5_block2_add - Trainable: True\n",
            "Layer 164: conv5_block2_out - Trainable: True\n",
            "Layer 165: conv5_block3_1_conv - Trainable: True\n",
            "Layer 166: conv5_block3_1_bn - Trainable: True\n",
            "Layer 167: conv5_block3_1_relu - Trainable: True\n",
            "Layer 168: conv5_block3_2_conv - Trainable: True\n",
            "Layer 169: conv5_block3_2_bn - Trainable: True\n",
            "Layer 170: conv5_block3_2_relu - Trainable: True\n",
            "Layer 171: conv5_block3_3_conv - Trainable: True\n",
            "Layer 172: conv5_block3_3_bn - Trainable: True\n",
            "Layer 173: conv5_block3_add - Trainable: True\n",
            "Layer 174: conv5_block3_out - Trainable: True\n",
            "Layer 175: avg_pool - Trainable: True\n"
          ]
        }
      ]
    }
  ]
}